#include <float.h>
#include <math.h>
#include <stdio.h>

#include <algorithm>
#include <cmath>

#include "amir_cuda_util/cuda_util.h"

using namespace amirstan::cuda;
template<typename scalar_t>
__device__ scalar_t deformable_im2col_bilinear(
    const scalar_t* bottom_data, const int data_width, const int height, const int width, scalar_t h, scalar_t w)
{
    const int h_low  = floor(h);
    const int w_low  = floor(w);
    const int h_high = h_low + 1;
    const int w_high = w_low + 1;

    const scalar_t lh = h - h_low;
    const scalar_t lw = w - w_low;
    const scalar_t hh = 1 - lh, hw = 1 - lw;

    scalar_t v1 = 0;
    if (h_low >= 0 && w_low >= 0)
        v1 = bottom_data[h_low * data_width + w_low];
    scalar_t v2 = 0;
    if (h_low >= 0 && w_high <= width - 1)
        v2 = bottom_data[h_low * data_width + w_high];
    scalar_t v3 = 0;
    if (h_high <= height - 1 && w_low >= 0)
        v3 = bottom_data[h_high * data_width + w_low];
    scalar_t v4 = 0;
    if (h_high <= height - 1 && w_high <= width - 1)
        v4 = bottom_data[h_high * data_width + w_high];

    scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;

    scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);
    return val;
}

template<typename scalar_t>
__global__ void deformable_im2col_gpu_kernel(const int       n,
                                             const scalar_t* data_im,
                                             const scalar_t* data_offset,
                                             const int       height,
                                             const int       width,
                                             const int       kernel_h,
                                             const int       kernel_w,
                                             const int       pad_h,
                                             const int       pad_w,
                                             const int       stride_h,
                                             const int       stride_w,
                                             const int       dilation_h,
                                             const int       dilation_w,
                                             const int       channel_per_deformable_group,
                                             const int       batch_size,
                                             const int       num_channels,
                                             const int       deformable_group,
                                             const int       height_col,
                                             const int       width_col,
                                             scalar_t*       data_col)
{
    CUDA_KERNEL_LOOP(index, n)
    {
        // index index of output matrix
        const int w_col = index % width_col;
        const int h_col = (index / width_col) % height_col;
        const int b_col = (index / width_col / height_col) % batch_size;
        const int c_im  = (index / width_col / height_col) / batch_size;
        const int c_col = c_im * kernel_h * kernel_w;

        // compute deformable group index
        const int deformable_group_index = c_im / channel_per_deformable_group;

        const int h_in         = h_col * stride_h - pad_h;
        const int w_in         = w_col * stride_w - pad_w;
        scalar_t* data_col_ptr = data_col + ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;
        // const scalar_t* data_im_ptr = data_im + ((b_col * num_channels + c_im) *
        // height + h_in) * width + w_in;
        const scalar_t* data_im_ptr = data_im + (b_col * num_channels + c_im) * height * width;
        const scalar_t* data_offset_ptr =
            data_offset
            + (b_col * deformable_group + deformable_group_index) * 2 * kernel_h * kernel_w * height_col * width_col;

        for (int i = 0; i < kernel_h; ++i) {
            for (int j = 0; j < kernel_w; ++j) {
                const int data_offset_h_ptr = ((2 * (i * kernel_w + j)) * height_col + h_col) * width_col + w_col;
                const int data_offset_w_ptr = ((2 * (i * kernel_w + j) + 1) * height_col + h_col) * width_col + w_col;
                const scalar_t offset_h     = data_offset_ptr[data_offset_h_ptr];
                const scalar_t offset_w     = data_offset_ptr[data_offset_w_ptr];
                scalar_t       val          = static_cast<scalar_t>(0);
                const scalar_t h_im         = h_in + i * dilation_h + offset_h;
                const scalar_t w_im         = w_in + j * dilation_w + offset_w;
                if (h_im > -1 && w_im > -1 && h_im < height && w_im < width) {
                    // const scalar_t map_h = i * dilation_h + offset_h;
                    // const scalar_t map_w = j * dilation_w + offset_w;
                    // const int cur_height = height - h_in;
                    // const int cur_width = width - w_in;
                    // val = deformable_im2col_bilinear(data_im_ptr, width, cur_height,
                    // cur_width, map_h, map_w);
                    val = deformable_im2col_bilinear(data_im_ptr, width, height, width, h_im, w_im);
                }
                *data_col_ptr = val;
                data_col_ptr += batch_size * height_col * width_col;
            }
        }
    }
}

template<typename scalar_t>
__device__ scalar_t dmcn_im2col_bilinear(
    const scalar_t* bottom_data, const int data_width, const int height, const int width, scalar_t h, scalar_t w)
{
    int h_low  = floor(h);
    int w_low  = floor(w);
    int h_high = h_low + 1;
    int w_high = w_low + 1;

    scalar_t lh = h - h_low;
    scalar_t lw = w - w_low;
    scalar_t hh = 1 - lh, hw = 1 - lw;

    scalar_t v1 = 0;
    if (h_low >= 0 && w_low >= 0)
        v1 = bottom_data[h_low * data_width + w_low];
    scalar_t v2 = 0;
    if (h_low >= 0 && w_high <= width - 1)
        v2 = bottom_data[h_low * data_width + w_high];
    scalar_t v3 = 0;
    if (h_high <= height - 1 && w_low >= 0)
        v3 = bottom_data[h_high * data_width + w_low];
    scalar_t v4 = 0;
    if (h_high <= height - 1 && w_high <= width - 1)
        v4 = bottom_data[h_high * data_width + w_high];

    scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;

    scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);
    return val;
}

template<typename scalar_t>
__global__ void modulated_deformable_im2col_gpu_kernel(const int       n,
                                                       const scalar_t* data_im,
                                                       const scalar_t* data_offset,
                                                       const scalar_t* data_mask,
                                                       const int       height,
                                                       const int       width,
                                                       const int       kernel_h,
                                                       const int       kernel_w,
                                                       const int       pad_h,
                                                       const int       pad_w,
                                                       const int       stride_h,
                                                       const int       stride_w,
                                                       const int       dilation_h,
                                                       const int       dilation_w,
                                                       const int       channel_per_deformable_group,
                                                       const int       batch_size,
                                                       const int       num_channels,
                                                       const int       deformable_group,
                                                       const int       height_col,
                                                       const int       width_col,
                                                       scalar_t*       data_col)
{
    CUDA_KERNEL_LOOP(index, n)
    {
        // index index of output matrix
        const int w_col = index % width_col;
        const int h_col = (index / width_col) % height_col;
        const int b_col = (index / width_col / height_col) % batch_size;
        const int c_im  = (index / width_col / height_col) / batch_size;
        const int c_col = c_im * kernel_h * kernel_w;

        // compute deformable group index
        const int deformable_group_index = c_im / channel_per_deformable_group;

        const int h_in = h_col * stride_h - pad_h;
        const int w_in = w_col * stride_w - pad_w;

        scalar_t* data_col_ptr = data_col + ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;
        // const float* data_im_ptr = data_im + ((b_col * num_channels + c_im) *
        // height + h_in) * width + w_in;
        const scalar_t* data_im_ptr = data_im + (b_col * num_channels + c_im) * height * width;
        const scalar_t* data_offset_ptr =
            data_offset
            + (b_col * deformable_group + deformable_group_index) * 2 * kernel_h * kernel_w * height_col * width_col;

        const scalar_t* data_mask_ptr =
            data_mask
            + (b_col * deformable_group + deformable_group_index) * kernel_h * kernel_w * height_col * width_col;

        for (int i = 0; i < kernel_h; ++i) {
            for (int j = 0; j < kernel_w; ++j) {
                const int data_offset_h_ptr = ((2 * (i * kernel_w + j)) * height_col + h_col) * width_col + w_col;
                const int data_offset_w_ptr = ((2 * (i * kernel_w + j) + 1) * height_col + h_col) * width_col + w_col;
                const int data_mask_hw_ptr  = ((i * kernel_w + j) * height_col + h_col) * width_col + w_col;
                const scalar_t offset_h     = data_offset_ptr[data_offset_h_ptr];
                const scalar_t offset_w     = data_offset_ptr[data_offset_w_ptr];
                const scalar_t mask         = data_mask_ptr[data_mask_hw_ptr];
                scalar_t       val          = static_cast<scalar_t>(0);
                const scalar_t h_im         = h_in + i * dilation_h + offset_h;
                const scalar_t w_im         = w_in + j * dilation_w + offset_w;
                // if (h_im >= 0 && w_im >= 0 && h_im < height && w_im < width) {
                if (h_im > -1 && w_im > -1 && h_im < height && w_im < width) {
                    // const float map_h = i * dilation_h + offset_h;
                    // const float map_w = j * dilation_w + offset_w;
                    // const int cur_height = height - h_in;
                    // const int cur_width = width - w_in;
                    // val = dmcn_im2col_bilinear(data_im_ptr, width, cur_height,
                    // cur_width, map_h, map_w);
                    val = dmcn_im2col_bilinear(data_im_ptr, width, height, width, h_im, w_im);
                }
                *data_col_ptr = val * mask;
                data_col_ptr += batch_size * height_col * width_col;
                // data_col_ptr += height_col * width_col;
            }
        }
    }
}

template<typename scalar_t>
__global__ void
output_add_bias_kernel(scalar_t* output, scalar_t* bias, size_t step_batch, size_t step_channel, size_t n)
{
    CUDA_KERNEL_LOOP(index, n)
    {
        output[index] += bias[(index % step_batch) / step_channel];
    }
}

void deformable_im2col(float*       data_input,
                       float*       data_offset,
                       const int    channels,
                       const int    height,
                       const int    width,
                       const int    ksize_h,
                       const int    ksize_w,
                       const int    pad_h,
                       const int    pad_w,
                       const int    stride_h,
                       const int    stride_w,
                       const int    dilation_h,
                       const int    dilation_w,
                       const int    parallel_imgs,
                       const int    deformable_group,
                       float*       data_col,
                       cudaStream_t stream)
{
    // num_axes should be smaller than block size
    // todo: check parallel_imgs is correctly passed in
    int height_col                   = (height + 2 * pad_h - (dilation_h * (ksize_h - 1) + 1)) / stride_h + 1;
    int width_col                    = (width + 2 * pad_w - (dilation_w * (ksize_w - 1) + 1)) / stride_w + 1;
    int num_kernels                  = channels * height_col * width_col * parallel_imgs;
    int channel_per_deformable_group = channels / deformable_group;

    deformable_im2col_gpu_kernel<float>
        <<<GET_BLOCKS(num_kernels), CUDA_NUM_THREADS, 0, stream>>>(num_kernels,
                                                                   data_input,
                                                                   data_offset,
                                                                   height,
                                                                   width,
                                                                   ksize_h,
                                                                   ksize_w,
                                                                   pad_h,
                                                                   pad_w,
                                                                   stride_h,
                                                                   stride_w,
                                                                   dilation_h,
                                                                   dilation_w,
                                                                   channel_per_deformable_group,
                                                                   parallel_imgs,
                                                                   channels,
                                                                   deformable_group,
                                                                   height_col,
                                                                   width_col,
                                                                   data_col);

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        printf("error in deformable_im2col: %s\n", cudaGetErrorString(err));
    }
}

void modulated_deformable_im2col_cuda(const float* data_im_,
                                      const float* data_offset_,
                                      const float* data_mask_,
                                      const int    batch_size,
                                      const int    channels,
                                      const int    height_im,
                                      const int    width_im,
                                      const int    height_col,
                                      const int    width_col,
                                      const int    kernel_h,
                                      const int    kenerl_w,
                                      const int    pad_h,
                                      const int    pad_w,
                                      const int    stride_h,
                                      const int    stride_w,
                                      const int    dilation_h,
                                      const int    dilation_w,
                                      const int    deformable_group,
                                      float*       data_col_,
                                      cudaStream_t stream)
{
    // num_axes should be smaller than block size
    const int channel_per_deformable_group = channels / deformable_group;
    const int num_kernels                  = channels * batch_size * height_col * width_col;

    modulated_deformable_im2col_gpu_kernel<<<GET_BLOCKS(num_kernels), CUDA_NUM_THREADS, 0, stream>>>(
        num_kernels,
        data_im_,
        data_offset_,
        data_mask_,
        height_im,
        width_im,
        kernel_h,
        kenerl_w,
        pad_h,
        pad_w,
        stride_h,
        stride_w,
        dilation_h,
        dilation_w,
        channel_per_deformable_group,
        batch_size,
        channels,
        deformable_group,
        height_col,
        width_col,
        data_col_);

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        printf("error in modulated_deformable_im2col_cuda: %s\n", cudaGetErrorString(err));
    }
}

template<typename scalar_t>
void output_add_bias(
    scalar_t* output, scalar_t* bias, size_t batch, size_t channel, size_t height, size_t width, cudaStream_t stream)
{
    size_t step_channel = height * width;
    size_t step_batch   = step_channel * channel;
    size_t n            = step_batch * batch;
    output_add_bias_kernel<<<GET_BLOCKS(n), CUDA_NUM_THREADS, 0, stream>>>(output, bias, step_batch, step_channel, n);
}

template void output_add_bias<float>(
    float* output, float* bias, size_t batch_size, size_t channel, size_t height, size_t width, cudaStream_t stream);

void tensorPermute(float* dst, float* src, int* src_size, int* permute, int src_dim, cudaStream_t stream)
{
    amirstan::cuda::memcpyPermute(dst, src, src_size, permute, src_dim, stream);
}
